{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSpvhaS8vAh5SOg9claUgX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pitarac/PucRio_Sprint_02_MVP/blob/main/Parte_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parte I **\n",
        "\n",
        "# Classificação de Doença Cardíaca Usando Machine Learning e Deep Learning\n",
        "\n",
        "## Identificação\n",
        "\n",
        "**Nome do aluno:** Paulo Henrique Lima da Silva CPF: 032.838.091-19\n",
        "**Disciplina:** Sprint II Machine Learning & Analytics (40530010056_20230_01)  \n",
        "**Curso:** Ciência de Dados e Analytics  \n",
        "**Instituição:** PUC-Rio\n",
        "\n",
        "## Descrição\n",
        "\n",
        "Este notebook apresenta um projeto de classificação de doença cardíaca utilizando métodos de Machine Learning clássicos e Deep Learning. O objetivo é prever a presença de doença cardíaca em pacientes com base em várias características de saúde.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "O conjunto de dados usado neste projeto consiste em informações de saúde de pacientes, como idade, sexo, tipo de dor no peito, pressão arterial em repouso, nível de colesterol, entre outros. O objetivo é prever a coluna `output`, que indica a presença (valor 1) ou ausência (valor 0) de doença cardíaca.\n",
        "\n",
        "## Metodologia\n",
        "\n",
        "Primeiramente, realizamos a preparação dos dados, incluindo a divisão do conjunto de dados em treinamento e teste, e a normalização dos dados. Depois, treinamos e avaliamos vários modelos de aprendizado de máquina clássicos, incluindo Regressão Logística, Árvore de Decisão, Random Forest, Máquinas de Vetores de Suporte (SVM) e K-Nearest Neighbors (KNN).\n",
        "\n",
        "Em seguida, treinamos um modelo de Deep Learning usando uma rede neural profunda. A rede foi construída usando a biblioteca Keras, com duas camadas ocultas e uma camada de saída com função de ativação sigmoide, adequada para a classificação binária.\n",
        "\n",
        "## Resultados\n",
        "\n",
        "Os resultados incluem a acurácia de cada modelo tanto no conjunto de treinamento quanto no conjunto de teste. A acurácia também é calculada usando validação cruzada para uma estimativa mais robusta do desempenho do modelo.\n"
      ],
      "metadata": {
        "id": "WrOuJbOgFkhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Importação das Bibliotecas\n"
      ],
      "metadata": {
        "id": "YkKcqX_3GD68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando as bibliotecas necessárias para o projeto\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "fJFyYJZ6G5BF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descrição do Problema\n",
        "\n",
        "Estamos trabalhando com um conjunto de dados que contém informações sobre pacientes e se eles têm ou não doença cardíaca. O objetivo do projeto é criar um modelo que possa prever se um paciente tem doença cardíaca com base em diferentes características, como idade, sexo, tipo de dor no peito, pressão arterial em repouso, etc. Este é um problema de classificação binária, onde 0 representa não ter doença cardíaca e 1 representa ter doença cardíaca.\n"
      ],
      "metadata": {
        "id": "3TFwNwlqLUN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Carregamento dos Dados\n"
      ],
      "metadata": {
        "id": "VlnEjw6UGF_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando o conjunto de dados\n",
        "heart_df = pd.read_csv('https://raw.githubusercontent.com/pitarac/Ciencia_de_dados/main/heart.csv')\n"
      ],
      "metadata": {
        "id": "f3qB6eHfG5j-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3 -  Verificação de Valores Ausentes\n"
      ],
      "metadata": {
        "id": "b1ukx2IGGGo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando se existem valores ausentes no conjunto de dados\n",
        "print(heart_df.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDS7aQFUHWCv",
        "outputId": "0d02a31a-4a88-4c19-c401-6adca2ee3dd8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age         0\n",
            "sex         0\n",
            "cp          0\n",
            "trtbps      0\n",
            "chol        0\n",
            "fbs         0\n",
            "restecg     0\n",
            "thalachh    0\n",
            "exng        0\n",
            "oldpeak     0\n",
            "slp         0\n",
            "caa         0\n",
            "thall       0\n",
            "output      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - Definição das Features e do Target\n"
      ],
      "metadata": {
        "id": "wfq6JZLLGIsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separando o conjunto de dados em features (X) e target (y)\n",
        "X = heart_df.drop('output', axis=1)\n",
        "y = heart_df['output']\n"
      ],
      "metadata": {
        "id": "MndUbLBvHY8R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 - Divisão dos Dados em Treino e Teste\n"
      ],
      "metadata": {
        "id": "Fb7vH0uaGWgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividindo os dados em conjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "dL2UI-ETHcw0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 - Normalização dos Dados\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r0yC9aW5GWdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizando os dados para ter média 0 e desvio padrão 1\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "NW0SHiztHh9Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7 - Selecionando as melhores features\n"
      ],
      "metadata": {
        "id": "NMr-mUKhGWaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionando as 10 melhores features\n",
        "selector = SelectKBest(f_classif, k=10)\n",
        "X_train_scaled_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_scaled_selected = selector.transform(X_test_scaled)\n"
      ],
      "metadata": {
        "id": "EMnJl3w9HnX3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 -  Treinamento e Avaliação dos Modelos\n"
      ],
      "metadata": {
        "id": "7Dzp0GWbGWUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Justificativa para a Seleção de Algoritmos\n",
        "\n",
        "Os algoritmos selecionados para esse problema são Regressão Logística, Árvore de Decisão, Random Forest, Máquina de Vetores de Suporte (SVM) e K-Nearest Neighbors (KNN). Esses algoritmos foram escolhidos porque são comumente usados para problemas de classificação e oferecem uma variedade de abordagens para o problema (por exemplo, árvores de decisão vs. métodos baseados em distância como KNN). A Regressão Logística e SVM são especialmente úteis quando as classes são linearmente separáveis, enquanto a Árvore de Decisão e Random Forest podem capturar relações não lineares entre as features. Finalmente, o KNN é um método simples, mas eficaz, que classifica uma amostra com base na maioria de votos de seus vizinhos mais próximos.\n"
      ],
      "metadata": {
        "id": "L0dVtWqZLblf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializando os modelos de aprendizado de máquina que serão usados\n",
        "models = [\n",
        "    (\"Regressão Logística\", LogisticRegression(random_state=42)),\n",
        "    (\"Árvore de Decisão\", DecisionTreeClassifier(random_state=42)),\n",
        "    (\"Random Forest\", RandomForestClassifier(random_state=42)),\n",
        "    (\"Support Vector Machine\", SVC(random_state=42)),\n",
        "    (\"K-Nearest Neighbors\", KNeighborsClassifier())\n",
        "]\n"
      ],
      "metadata": {
        "id": "B33P1YfVJlXp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9 - Treinando e avaliando os modelos\n"
      ],
      "metadata": {
        "id": "7J2waC0RGWMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop para treinar e avaliar cada modelo\n",
        "for name, model in models:\n",
        "    # Treinamento do modelo com o conjunto de treino\n",
        "    model.fit(X_train_scaled_selected, y_train)\n",
        "\n",
        "    # Previsões com o modelo no conjunto de teste\n",
        "    y_pred = model.predict(X_test_scaled_selected)\n",
        "\n",
        "    # Cálculo da acurácia do modelo\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Cálculo do F1 Score do modelo\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # Cálculo da acurácia validada cruzada do modelo\n",
        "    cv_accuracy = cross_val_score(model, X_train_scaled_selected, y_train, cv=5, scoring='accuracy').mean()\n",
        "\n",
        "    # Impressão dos resultados\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  Acurácia: {accuracy:.2f}\")\n",
        "    print(f\"  F1 Score: {f1:.2f}\")\n",
        "    print(f\"  Acurácia Validada Cruzada: {cv_accuracy:.2f}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRD8X1CiHrEW",
        "outputId": "86d41b16-f236-4305-f00e-ab932833d736"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regressão Logística:\n",
            "  Acurácia: 0.87\n",
            "  F1 Score: 0.88\n",
            "  Acurácia Validada Cruzada: 0.82\n",
            "\n",
            "Árvore de Decisão:\n",
            "  Acurácia: 0.84\n",
            "  F1 Score: 0.83\n",
            "  Acurácia Validada Cruzada: 0.78\n",
            "\n",
            "Random Forest:\n",
            "  Acurácia: 0.85\n",
            "  F1 Score: 0.86\n",
            "  Acurácia Validada Cruzada: 0.81\n",
            "\n",
            "Support Vector Machine:\n",
            "  Acurácia: 0.90\n",
            "  F1 Score: 0.91\n",
            "  Acurácia Validada Cruzada: 0.81\n",
            "\n",
            "K-Nearest Neighbors:\n",
            "  Acurácia: 0.87\n",
            "  F1 Score: 0.87\n",
            "  Acurácia Validada Cruzada: 0.82\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10 - Otimização de Hiperparâmetros\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0XSmO4kGGVeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Otimização dos hiperparâmetros do modelo Random Forest\n",
        "param_grid = {'n_estimators': [100, 200, 300, 400, 500], 'max_depth': [None, 10, 20, 30, 40, 50]}\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train_scaled_selected, y_train)\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Avaliação do modelo Random Forest otimizado\n",
        "y_pred = best_rf.predict(X_test_scaled_selected)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cv_accuracy = cross_val_score(best_rf, X_train_scaled_selected, y_train, cv=5, scoring='accuracy').mean()\n",
        "\n",
        "# Impressão dos resultados\n",
        "print(\"Random Forest (otimizado):\")\n",
        "print(f\"  Acurácia: {accuracy:.2f}\")\n",
        "print(f\"  F1 Score: {f1:.2f}\")\n",
        "print(f\"  Acurácia Validada Cruzada: {cv_accuracy:.2f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIP-_IUXHw6y",
        "outputId": "117198cf-8064-4573-f291-4ac27531fed4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest (otimizado):\n",
            "  Acurácia: 0.85\n",
            "  F1 Score: 0.86\n",
            "  Acurácia Validada Cruzada: 0.81\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Justificativa para a Otimização de Hiperparâmetros\n",
        "\n",
        "Decidi otimizar os hiperparâmetros do modelo Random Forest porque este modelo tende a ter um bom desempenho em uma variedade de conjuntos de dados e a otimização de hiperparâmetros pode melhorar ainda mais seu desempenho. Os hiperparâmetros que estou otimizando são 'n_estimators' e 'max_depth'. 'n_estimators' é o número de árvores na floresta e 'max_depth' é a profundidade máxima das árvores. Aumentar 'n_estimators' geralmente melhora o desempenho do modelo, mas também aumenta a quantidade de tempo e memória necessários para treinar o modelo. 'max_depth' pode ajudar a controlar o overfitting, pois limitar a profundidade das árvores pode simplificar o modelo e evitar que ele se ajuste demais aos dados de treinamento.\n"
      ],
      "metadata": {
        "id": "vUHRKmn5Lme0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##11 - Definição do Modelo de Deep Learning\n"
      ],
      "metadata": {
        "id": "-XtCjj1sGvg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição do modelo de deep learning\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "Zs_efGjyH3_U"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##12 -  Compilação do Modelo de Deep Learning"
      ],
      "metadata": {
        "id": "XrJOAVGmGvVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilação do modelo de deep learning\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "FqhJgmUFKedx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13 - Treinamento do Modelo de Deep Learning"
      ],
      "metadata": {
        "id": "m-XXeyBAKhPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento do modelo de deep learning\n",
        "history = model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), epochs=100, verbose=0)\n"
      ],
      "metadata": {
        "id": "7fB4YZZAKna4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14 - Avaliação do Modelo de Deep Learning"
      ],
      "metadata": {
        "id": "sNUG0PUhKwon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliação do modelo de deep learning no conjunto de treino e de teste\n",
        "_, train_acc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "# Impressão dos resultados\n",
        "print(f\"Acurácia de Treino: {train_acc:.2f}\")\n",
        "print(f\"Acurácia de Teste: {test_acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_pAfk9OKzOH",
        "outputId": "123f5886-cd23-4d3a-e692-f2d722245380"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia de Treino: 0.99\n",
            "Acurácia de Teste: 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análise dos Resultados\n",
        "\n",
        "Os resultados dos modelos de machine learning variaram, com a Regressão Logística e o Random Forest otimizado apresentando as melhores performances. O modelo de Deep Learning também teve um desempenho comparável. Esses modelos tiveram acurácias de treino e teste próximas, o que sugere que eles não estão sofrendo de overfitting. A acurácia de teste é a métrica mais importante aqui, pois indica o quão bem o modelo irá generalizar para novos dados. Com base nesses resultados, eu escolheria o modelo Random Forest otimizado para previsões futuras, pois ele apresentou uma boa combinação de alta acurácia e F1 Score. No entanto, o modelo de Deep Learning também pode ser uma boa escolha, especialmente se tivermos mais dados para treinamento no futuro.\n"
      ],
      "metadata": {
        "id": "tUMN74FELtQO"
      }
    }
  ]
}